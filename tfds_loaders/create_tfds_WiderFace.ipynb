{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## WIDER Faces Dataset\n",
    "\n",
    "The most diverse and larget face detection dataset.\n",
    "\n",
    "Source: http://shuoyang1213.me/WIDERFACE/\n",
    "\n",
    "The WIDER Faces dataset provides text files for annotations and gets a diverse set of images as jpg's under discrete categories.\n",
    "**dob**: date of birth (Matlab serial date number)  \n",
    "**photo_taken**: year when the photo was taken  \n",
    "**full_path**: path to file  \n",
    "**gender**: 0 for female and 1 for male, NaN if unknown  \n",
    "**name**: name of the celebrity  \n",
    "**face_location**: location of the face (bounding box)  \n",
    "**face_score**: detector score (the higher the better). Inf implies that no face was found in the image and the face_location then just returns the entire image  \n",
    "**second_face_score**: detector score of the face with the second highest score. This is useful to ignore images with more than one face. second_face_score is NaN if no second face was detected.  \n",
    "**celeb_names**: list of all celebrity names  \n",
    "**celeb_id**: index of celebrity name  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nick\\Desktop\\image\\face_detection\n",
      "['.ipynb_checkpoints', 'create_tfds_WiderFace.ipynb', 'data']\n"
     ]
    }
   ],
   "source": [
    "# i like to make data a symlink to an external drive using 'mklink .../data <datadir>'\n",
    "print(os.getcwd())\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WIDER_val.zip',\n",
       " 'WIDER_test.zip',\n",
       " 'WIDER_train.zip',\n",
       " 'WIDER_train',\n",
       " 'WIDER_val',\n",
       " 'WIDER_test',\n",
       " 'wider_face_split.zip',\n",
       " 'wider_face_split']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadir = 'D:\\\\face_detection\\\\WIDERFaces'\n",
    "os.listdir(datadir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete Categories:\n",
      "\n",
      "['0--Parade', '1--Handshaking', '10--People_Marching', '11--Meeting', '12--Group', '13--Interview', '14--Traffic', '15--Stock_Market', '16--Award_Ceremony', '17--Ceremony', '18--Concerts', '19--Couple', '2--Demonstration', '20--Family_Group', '21--Festival', '22--Picnic', '23--Shoppers', '24--Soldier_Firing', '25--Soldier_Patrol', '26--Soldier_Drilling', '27--Spa', '28--Sports_Fan', '29--Students_Schoolkids', '3--Riot', '30--Surgeons', '31--Waiter_Waitress', '32--Worker_Laborer', '33--Running', '34--Baseball', '35--Basketball', '36--Football', '37--Soccer', '38--Tennis', '39--Ice_Skating', '4--Dancing', '40--Gymnastics', '41--Swimming', '42--Car_Racing', '43--Row_Boat', '44--Aerobics', '45--Balloonist', '46--Jockey', '47--Matador_Bullfighter', '48--Parachutist_Paratrooper', '49--Greeting', '5--Car_Accident', '50--Celebration_Or_Party', '51--Dresses', '52--Photographers', '53--Raid', '54--Rescue', '55--Sports_Coach_Trainer', '56--Voter', '57--Angler', '58--Hockey', '59--people--driving--car', '6--Funeral', '61--Street_Battle', '7--Cheering', '8--Election_Campain', '9--Press_Conference']\n"
     ]
    }
   ],
   "source": [
    "print('Discrete Categories:\\n')\n",
    "print(os.listdir(os.path.join(datadir, 'WIDER_train', 'images')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12880 training images\n",
      "3226 validation images\n",
      "16097 test images\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['D:\\\\face_detection\\\\WIDERFaces\\\\WIDER_train\\\\images\\\\0--Parade\\\\0_Parade_marchingband_1_100.jpg',\n",
       " 'D:\\\\face_detection\\\\WIDERFaces\\\\WIDER_train\\\\images\\\\0--Parade\\\\0_Parade_marchingband_1_1015.jpg',\n",
       " 'D:\\\\face_detection\\\\WIDERFaces\\\\WIDER_train\\\\images\\\\0--Parade\\\\0_Parade_marchingband_1_1018.jpg',\n",
       " 'D:\\\\face_detection\\\\WIDERFaces\\\\WIDER_train\\\\images\\\\0--Parade\\\\0_Parade_marchingband_1_1022.jpg',\n",
       " 'D:\\\\face_detection\\\\WIDERFaces\\\\WIDER_train\\\\images\\\\0--Parade\\\\0_Parade_marchingband_1_1030.jpg']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images = [x for x in glob.glob(os.path.join(datadir, 'WIDER_train', '**',  '*.jpg'), recursive=True)]\n",
    "validation_images = [x for x in glob.glob(os.path.join(datadir, 'WIDER_val', '**',  '*.jpg'), recursive=True)]\n",
    "test_images = [x for x in glob.glob(os.path.join(datadir, 'WIDER_test', '**',  '*.jpg'), recursive=True)]\n",
    "\n",
    "print(f'{len(train_images)} training images')\n",
    "print(f'{len(validation_images)} validation images')\n",
    "print(f'{len(test_images)} test images')\n",
    "train_images[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#see IDMB Faces example for .mat example\n",
    "# we'll skip test set since it isnt labeled \n",
    "with open(os.path.join(datadir, 'wider_face_split', 'wider_face_train_bbx_gt.txt'), 'r') as f:\n",
    "    meta_train = f.read()\n",
    "with open(os.path.join(datadir, 'wider_face_split', 'wider_face_val_bbx_gt.txt'), 'r') as f:\n",
    "    meta_val = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0--Parade/0_Parade_marchingband_1_849.jpg',\n",
       " '1',\n",
       " '449 330 122 149 0 0 0 0 0 0 ',\n",
       " '0--Parade/0_Parade_Parade_0_904.jpg',\n",
       " '1']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_train = meta_train.split('\\n')\n",
    "meta_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#chunkify\n",
    "split_train=[]\n",
    "y=[]\n",
    "for line in meta_train:\n",
    "    if '.jpg' in line:\n",
    "        split_train.append(y)\n",
    "        y=[line]\n",
    "    else:\n",
    "        y.append(line)\n",
    "split_train = split_train[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "formatted_ann=[]\n",
    "for meta in split_train:\n",
    "    #print(ann)\n",
    "    out = {'image/filename': meta[0], 'faces': []}\n",
    "    annotations = meta[2:]\n",
    "    for ann in annotations:\n",
    "        ann = ann.split(' ', 12)\n",
    "        face_annotation = {'bbox': ann[:4]} # (x1, y1, w, h)\n",
    "        face_annotation['blur'] = ann[4]\n",
    "        face_annotation['expression'] = ann[5]\n",
    "        face_annotation['illumination'] = ann[6]\n",
    "        face_annotation['invalid'] = ann[7]\n",
    "        face_annotation['occlusion'] = ann[8]\n",
    "        face_annotation['pose'] = ann[9]\n",
    "        out['faces'].append(face_annotation)\n",
    "    formatted_ann.append(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Define the Dataset with `GeneratorBasedBuilder`\n",
    "\n",
    "Most datasets subclass `tfds.core.GeneratorBasedBuilder`, which is a subclass of `tfds.core.DatasetBuilder` that simplifies defining a dataset. It works well for datasets that can be generated on a single machine. Its subclasses implement:\n",
    "\n",
    "* `_info`: builds the DatasetInfo object describing the dataset\n",
    "\n",
    "\n",
    "* `_split_generators`: downloads the source data and defines the dataset splits\n",
    "\n",
    "\n",
    "* `_generate_examples`: yields (key, example) tuples in the dataset from the source data\n",
    "\n",
    "In this exercise, you will use the `GeneratorBasedBuilder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.75"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xmin, ymin, bbox_width, bbox_height = np.array(['1', '2', '.5', '.75'], dtype='float32')\n",
    "xmin + bbox_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "449 330 122 149 0 0 0 0 0 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[488.906, 373.643,   0.   ],\n",
       "       [542.089, 376.442,   0.   ],\n",
       "       [515.031, 412.83 ,   0.   ],\n",
       "       [485.174, 425.893,   0.   ],\n",
       "       [538.357, 431.491,   0.   ]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann = \"449 330 122 149 488.906 373.643 0.0 542.089 376.442 0.0 515.031 412.83 0.0 485.174 425.893 0.0 538.357 431.491 0.0 0.82\"\n",
    "ann = ann.split()\n",
    "np.array(ann[4:19], dtype='float32').reshape((5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%%writefile tensorflow_datasets/object_detection/wider_face.py\n",
    "\n",
    "# coding=utf-8\n",
    "# Copyright 2019 The TensorFlow Datasets Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"IMDB Faces dataset.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "_DESCRIPTION = \"\"\"\\\n",
    "Since the publicly available face image datasets are often of small to medium size, rarely exceeding tens of thousands of images, and often without age information we decided to collect a large dataset of celebrities. For this purpose, we took the list of the most popular 100,000 actors as listed on the IMDb website and (automatically) crawled from their profiles date of birth, name, gender and all images related to that person. Additionally we crawled all profile images from pages of people from Wikipedia with the same meta information. We removed the images without timestamp (the date when the photo was taken). Assuming that the images with single faces are likely to show the actor and that the timestamp and date of birth are correct, we were able to assign to each such image the biological (real) age. Of course, we can not vouch for the accuracy of the assigned age information. Besides wrong timestamps, many images are stills from movies - movies that can have extended production times. In total we obtained 460,723 face images from 20,284 celebrities from IMDb and 62,328 from Wikipedia, thus 523,051 in total.\n",
    "\n",
    "As some of the images (especially from IMDb) contain several people we only use the photos where the second strongest face detection is below a threshold. For the network to be equally discriminative for all ages, we equalize the age distribution for training. For more details please the see the paper.\n",
    "\"\"\"\n",
    "\n",
    "_PROJECT_URL = 'http://shuoyang1213.me/WIDERFACE/'\n",
    "\n",
    "# _WIDER_TRAIN_URL = ('https://drive.google.com/uc?export=download&'\n",
    "#                     'id=0B6eKvaijfFUDQUUwd21EckhUbWs')\n",
    "\n",
    "# _WIDER_VAL_URL = ('https://drive.google.com/uc?export=download&'\n",
    "#                   'id=0B6eKvaijfFUDd3dIRmpvSk8tLUk')\n",
    "\n",
    "# _WIDER_TEST_URL = ('https://drive.google.com/uc?export=download&'\n",
    "#                    'id=0B6eKvaijfFUDbW4tdGpaYjgzZkU')\n",
    "\n",
    "# _WIDER_ANNOT_URL = ('https://drive.google.com/uc?export=download&'\n",
    "#                     'id=1sAl2oml7hK6aZRdgRjqQJsjV5CEr7nl4')\n",
    "\n",
    "_WIDER_TRAIN_URL = ('https://drive.google.com/file/d/0B6eKvaijfFUDQUUwd21EckhUbWs/view?usp=sharing')\n",
    "_WIDER_VAL_URL = ('https://drive.google.com/file/d/0B6eKvaijfFUDd3dIRmpvSk8tLUk/view?usp=sharing')\n",
    "_WIDER_TEST_URL = ('https://drive.google.com/file/d/0B6eKvaijfFUDbW4tdGpaYjgzZkU/view?usp=sharing')\n",
    "_WIDER_ANNOT_URL = ('http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/support/bbx_annotation/wider_face_split.zip')\n",
    "\n",
    "\n",
    "_CITATION = \"\"\"\n",
    "@inproceedings{yang2016wider,\n",
    "\tAuthor = {Yang, Shuo and Luo, Ping and Loy, Chen Change and Tang, Xiaoou},\n",
    "\tBooktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n",
    "\tTitle = {WIDER FACE: A Face Detection Benchmark},\n",
    "\tYear = {2016}}\n",
    "\"\"\"\n",
    "\n",
    "_DESCRIPTION = \"\"\"\n",
    "WIDER FACE dataset is a face detection benchmark dataset, of which images are \n",
    "selected from the publicly available WIDER dataset. We choose 32,203 images and \n",
    "label 393,703 faces with a high degree of variability in scale, pose and \n",
    "occlusion as depicted in the sample images. WIDER FACE dataset is organized \n",
    "based on 61 event classes. For each event class, we randomly select 40%/10%/50% \n",
    "data as training, validation and testing sets. We adopt the same evaluation \n",
    "metric employed in the PASCAL VOC dataset. Similar to MALF and Caltech datasets,\n",
    "we do not release bounding box ground truth for the test images. Users are \n",
    "required to submit final prediction files, which we shall proceed to evaluate.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class WiderFace(tfds.core.GeneratorBasedBuilder):\n",
    "    \"\"\"WIDER Face dataset.\"\"\"\n",
    "\n",
    "    VERSION = tfds.core.Version(\"0.1.1\")\n",
    "\n",
    "    def _info(self):\n",
    "        return tfds.core.DatasetInfo(\n",
    "            builder=self,\n",
    "            description=_DESCRIPTION,\n",
    "            # Describe the features of the dataset by following this url\n",
    "            # https://www.tensorflow.org/datasets/api_docs/python/tfds/features\n",
    "            features=tfds.features.FeaturesDict({\n",
    "                'image': tfds.features.Image(encoding_format='jpeg'),\n",
    "                'image/filename': tfds.features.Text(),\n",
    "                'faces': tfds.features.Sequence({\n",
    "                    'bbox': tfds.features.BBoxFeature(),\n",
    "                    'blur': tf.uint8,\n",
    "                    'expression': tf.bool,\n",
    "                    'illumination': tf.bool,\n",
    "                    'occlusion': tf.uint8,\n",
    "                    'pose': tf.bool,\n",
    "                    'invalid': tf.bool\n",
    "                })\n",
    "            }), \n",
    "            #supervised_keys=(\"image\", \"category\"),\n",
    "            homepage=_PROJECT_URL,\n",
    "            citation=_CITATION)\n",
    "\n",
    "    def _split_generators(self, dl_manager):\n",
    "        # Download the dataset and then extract it.\n",
    "        extracted_dirs = dl_manager.download_and_extract({\n",
    "            'wider_train': _WIDER_TRAIN_URL,\n",
    "            'wider_val': _WIDER_VAL_URL,\n",
    "            #'wider_test': _WIDER_TEST_URL,\n",
    "            'wider_annot': _WIDER_ANNOT_URL\n",
    "        })\n",
    "\n",
    "        # Parsing the mat file which contains the list of train images\n",
    "\n",
    "        return [\n",
    "           tfds.core.SplitGenerator(\n",
    "               name=tfds.Split.TRAIN,\n",
    "               gen_kwargs={\n",
    "                   'split': 'train',\n",
    "                   'extracted_dirs': extracted_dirs\n",
    "               }),\n",
    "            tfds.core.SplitGenerator(\n",
    "                name=tfds.Split.VALIDATION,\n",
    "                gen_kwargs={\n",
    "                    'split': 'val',\n",
    "                    'extracted_dirs': extracted_dirs\n",
    "                })\n",
    "            #excluding test data\n",
    "#             tfds.core.SplitGenerator(\n",
    "#                 name=tfds.Split.TEST,\n",
    "#                 gen_kwargs={\n",
    "#                     'split': 'test',\n",
    "#                     'extracted_dirs': extracted_dirs\n",
    "#                 })\n",
    "        ]\n",
    "\n",
    "    def _get_bounding_box_values(self, bbox_annotations, img_width, img_height):\n",
    "        \"\"\"Function to get normalized bounding box values.\n",
    "\n",
    "        Args:\n",
    "          bbox_annotations: list of bbox values in kitti format\n",
    "          img_width: image width\n",
    "          img_height: image height\n",
    "\n",
    "        Returns:\n",
    "          Normalized bounding box xmin, ymin, xmax, ymax values\n",
    "        \"\"\"\n",
    "        ymax = np.clip(ymin + hbox, a_min=0, a_max=img_height)\n",
    "        xmax = np.clip(xmin + wbox, a_min=0, a_max=img_width)\n",
    "        ymin = np.clip(ymin, a_min=0, a_max=img_height)\n",
    "        xmin = np.clip(xmin, a_min=0, a_max=img_width)\n",
    "\n",
    "        ymin = bbox_annotations[0] / img_height\n",
    "        xmin = bbox_annotations[1] / img_width\n",
    "        ymax = bbox_annotations[2] / img_height\n",
    "        xmax = bbox_annotations[3] / img_width\n",
    "        return ymin, xmin, ymax, xmax\n",
    "  \n",
    "    def _get_image_shape(self, image_path):\n",
    "        image = tf.io.read_file(image_path)\n",
    "        image = tf.image.decode_image(image, channels=3)\n",
    "        shape = image.shape[:2]\n",
    "        return shape\n",
    "    \n",
    "    def _parse_annotation_file(self, filename):\n",
    "        with tf.io.gfile.GFile(filename, 'r') as f:\n",
    "            meta = f.read()\n",
    "        meta = metea.split('\\n')\n",
    "        #chunkify\n",
    "        split_train=[]\n",
    "        y=[]\n",
    "        for line in meta_train:\n",
    "            if '.jpg' in line:\n",
    "                split_train.append(y)\n",
    "                y=[line]\n",
    "            else:\n",
    "                y.append(line)\n",
    "        split = split[1:]\n",
    "        \n",
    "        formatted_annotations=[]\n",
    "        for meta in split:\n",
    "            out = {'image/filename': meta[0], 'faces': []}\n",
    "            annotations = meta[2:]\n",
    "            for ann in annotations:\n",
    "                ann = ann.split(' ', 12)\n",
    "                face_annotation = {'bbox': ann[:4]} # (x1, y1, w, h)\n",
    "                face_annotation['blur'] = ann[4]\n",
    "                face_annotation['expression'] = ann[5]\n",
    "                face_annotation['illumination'] = ann[6]\n",
    "                face_annotation['invalid'] = ann[7]\n",
    "                face_annotation['occlusion'] = ann[8]\n",
    "                face_annotation['pose'] = ann[9]\n",
    "                out['faces'].append(face_annotation)\n",
    "            formatted_annotations.append(out)\n",
    "        return formatted_annotations\n",
    "\n",
    "    def _generate_examples(self, split, extracted_dir):\n",
    "        image_dir = os.path.join(extracted_dirs[f'wider_{split}'], f'WIDER_{split}', 'images')\n",
    "        annotation_dir = os.path.join(extracted_dirs['wider_annot'], 'wider_face_split', f'wider_face_{}_bbx_gt.text')\n",
    "        \n",
    "        annotations = self._parse_annotation_file(annotation_dir)\n",
    "        \n",
    "        for ann in annotations:\n",
    "            # this includes a category subdirectory (ex: 0--Parade\\0_Parade_marchingband_1_5.jpg)\n",
    "            img_pth = os.path.join(image_dir, ann['image/filename'])\n",
    "            img_width, img_height = self._get_image_shape(img_path)\n",
    "            #noramlize bounding pox points\n",
    "            ymin, xmin, ymax, xmax = self._get_bounding_box_values(ann['bbox'])\n",
    "            \n",
    "            record = {\n",
    "                'image': img_path,\n",
    "                'image/filename': ann['image/filename'],\n",
    "                'faces': {\n",
    "                    'bbox': tfds.features.BBox(xmin=xmin,\n",
    "                                               ymin=ymin,\n",
    "                                               xmax=xmax,\n",
    "                                               ymax=ymax),\n",
    "                    'blur': ann['blur'],\n",
    "                    'expression': ann['expression'],\n",
    "                    'illumination': ann['illumination'],\n",
    "                    'invalid': ann['invalid'], \n",
    "                    'occlusion': ann['occlusion'],\n",
    "                    'pose': ann['pose'],\n",
    "                }\n",
    "            }\n",
    "            # Yield a feature dictionary \n",
    "            yield filename, record\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfjs",
   "language": "python",
   "name": "tfjs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
